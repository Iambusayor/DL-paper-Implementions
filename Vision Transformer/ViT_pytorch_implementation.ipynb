{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-19T21:21:54.037414Z","iopub.execute_input":"2022-07-19T21:21:54.038210Z","iopub.status.idle":"2022-07-19T21:21:54.074500Z","shell.execute_reply.started":"2022-07-19T21:21:54.038057Z","shell.execute_reply":"2022-07-19T21:21:54.073253Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2022-07-19T21:21:54.080259Z","iopub.execute_input":"2022-07-19T21:21:54.081555Z","iopub.status.idle":"2022-07-19T21:21:56.204404Z","shell.execute_reply.started":"2022-07-19T21:21:54.081462Z","shell.execute_reply":"2022-07-19T21:21:56.202885Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class PatchEmbde(nn.Module):\n    \"\"\"Split image into patches and then embed them.\n    \n    Paramters\n    -----------\n    img_size: int\n        Size of the image (it is a squeeze).\n    \n    patch_size: int\n        Size of the patch (it is a squeeze).\n        \n    in_chans: int \n        Number of the input channels.\n        \n    embed_dim: int\n        The embedding dimension, it will determine how big of an embedding our patch is going to be.\n        \n    Attributes\n    -----------\n    n_patches: int\n        Number of patches inside the image.\n    proj: nn.Conv2d\n        Convolution layer that does both the splitting into patches and their embedding.\n    \"\"\"\n    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        \n        self.proj = nn.Conv2d(\n            in_chans,\n            embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n    \n    def forward(self, x):\n        \"\"\"Run forward pass.\n        \n        Parameters\n        ----------\n        x: torch.Tensor\n            Shape `(n_samples, in_chans, img_size, img_size)`.\n        \n        Returns\n        -------\n        torch.Tensor\n            Shape `(n_samples, n_patches, embed_dims)`.\n        \"\"\"\n        x = self.proj(\n            x,\n        ) # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n        x = x.flatten(2) # (n_samples, embed_dim, n_patches)\n        x = x.transpose(1, 2) # (n_samples, n_patches, embed_dim)\n        \n        return x\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-19T21:21:56.206331Z","iopub.execute_input":"2022-07-19T21:21:56.207024Z","iopub.status.idle":"2022-07-19T21:21:56.220665Z","shell.execute_reply.started":"2022-07-19T21:21:56.206980Z","shell.execute_reply":"2022-07-19T21:21:56.219633Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    \"\"\"Attention mechanism.\n    \n    Parameters\n    ----------\n    dim: int \n        The input and out dimension of per token features.\n    \n    n_heads: int\n        Number of attention heads.\n        \n    qkv_bias: bool\n        If True then we include bias to the query, key and value projections.\n        \n    attn_p: float\n        Dropout probability applied to the query, key, and value tensors.\n        \n    proj_p: float\n        Dropout probability applied to the output tensor.\n        \n    Attributes\n    ----------\n    scale: float\n        Normalizing constant for the dot product.\n    \n    qkv: nn.Linear\n        Linear projection for the query, key, value.\n        \n    attn_drop, proj_drop: nn.Dropout\n        Dropout layers\n    \"\"\"\n    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n        super().__init__()\n        self.n_heads = n_heads\n        self.dim = dim\n        self.head_dim = dim // n_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(dim, dim*3, bias, bias=qkv_bias)\n        self.attn_drop = nn.Dopout(attn_p)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_p)\n        \n    def forward(self, x):\n        \"\"\"Run forward pass.\n        \n        Paramters\n        ---------\n        x: torch.Tensor\n            Shape (n_samples, n_patches + 1, dim)`.\n        \n        Returns\n        -------\n        torch.Tensor\n            Shape `(n_samples, n_patches + 1, dim)`.\n        \"\"\"\n        n_samples, n_tokens, dim = x.shape\n        \n        if dim != self.dim:\n            raise ValueError\n            \n        qkv = self.qkv(x) # (n_samples, n_patches +1, 3 * dim)\n        qkv = qkv.reshape(\n            n_samples,n_tokens, 3, self.n_heads, self.head_dim\n        ) # (n_samples, n_patches + 1, 3, n_heads, head_dim)\n        qkv = qkv.permute(\n            2, 0, 3, 1, 4\n        ) # (3, n_samples, n_heads, n_patches + 1, head_dim)\n        \n        q, k, v = qkv[0], qkv[1], qkv[2]\n        k_t = k.transpose(-2, -1) # (n_samples, n_heads, head_dim, n_patches + 1)\n        dp = (\n            q @ k_t\n        ) * self.scale # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n        attn = dp.softmax(dim=-1) # (samples, n_heads, n_patches + 1, n_patches + 1)\n        attn = self.attn_drop(attn)\n        \n        weighted_avg = attn @ v # (n_samples, n_heads, n_patches + 1, head_dim)\n        weighted_avg = weighted_avg.transpose(\n            1, 2\n        ) # (n_samples, n_patches + 1, n_heads, head_dim)\n        weighted_avg = weighted_avg.flatten(2) # (n_samples, n_ptches + 1, dim)\n        x = self.proj_drop(x) # (n_samples, n_patches + 1, dim)\n        \n        return x ","metadata":{"execution":{"iopub.status.busy":"2022-07-19T21:21:56.223274Z","iopub.execute_input":"2022-07-19T21:21:56.224240Z","iopub.status.idle":"2022-07-19T21:21:56.241050Z","shell.execute_reply.started":"2022-07-19T21:21:56.224203Z","shell.execute_reply":"2022-07-19T21:21:56.239853Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    \"\"\"Multilayerm perceptron.\n    \n    Parameters\n    ----------\n    in_features: int\n        Number of input features.\n        \n    hidden_features: int\n        Number of nodes in the hidden layer.\n    \n    out_features: int\n        Number of output features.\n        \n    p: float\n        Dropout probability.\n    \n    Attributes\n    ----------\n    fc: nn.Linear\n        The First linear layer.\n    \n    act: nn.GELU\n        GELU activation functions.\n        \n    fc2: nn.Linear\n        The second linear layer.\n        \n    drop: nn.Dropout\n        Dropout layer.\n    \"\"\"\n    def __init__(self, in_features, hidden_features, out_features, p=0.):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(p)\n        \n    def forward(self, x):\n        \"\"\"Run forward pass.\n        \n        Parameters\n        ----------\n        x: torch.Tensor\n            Shape `(n_samples, n_patches + 1, in_features)`.\n            \n        Returns\n        -------\n        torch.Tensor\n            Shape `(n_samples, n_patches + 1, out_features)`\n        \"\"\"\n        x = self.fc1(\n            x\n        ) # (n_samples, n_patches + 1, hidden_features)\n        x = self.act(x)  # (n_samples, n_patches + 1, hidden_features) \n        x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n        x = self.fc2(x)  # (n_samples, n_patches + 1, hidden_features)\n        x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-07-19T21:21:56.242718Z","iopub.execute_input":"2022-07-19T21:21:56.243124Z","iopub.status.idle":"2022-07-19T21:21:56.260001Z","shell.execute_reply.started":"2022-07-19T21:21:56.243093Z","shell.execute_reply":"2022-07-19T21:21:56.258791Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    \"\"\"Transformer block.\n      \n    Parameters\n    ----------\n    dim: int\n        Embedding dimension.\n    \n    n_heads: int \n        Number of attention heads.\n    \n    mlp_ratio: float\n        Determines the hidden dimension size ofthe `MLP` module with respect to `dim`.\n        \n    qkv_bias: bool\n        If True then we include bias to the query, key and value projections.\n        \n    p, attn_p: float\n         Dropout probability.\n         \n    Attributes\n    ----------\n    norm1, norm2: LayerNorm\n        Layer normalization.\n        \n    attn: Attention\n        Attention module.\n    \n    mlp: MLP\n        MLP module.\n    \"\"\"\n    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n        self.attn = Attention(\n            dim,\n            n_heads=n_heads,\n            qkv_bias=qkv_bias,\n            attn_p=attn_p,\n            proj_p=p\n        )\n        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n        hidden_features = int(dim * mlp_ratio)\n        self.mlp = MLP(\n            in_features=dim,\n            hidden_features=hidden_features,\n            out_features=dim,\n        )\n    \n    def forward(self, x):\n        \"\"\"Run forward pass.\n        \n        Parameters\n        ----------\n        x: torch.Tensor\n            Shape `(n_sampes, n_patches + 1, dim)`.\n        \"\"\"\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-07-19T21:21:56.261480Z","iopub.execute_input":"2022-07-19T21:21:56.262071Z","iopub.status.idle":"2022-07-19T21:21:56.277546Z","shell.execute_reply.started":"2022-07-19T21:21:56.262039Z","shell.execute_reply":"2022-07-19T21:21:56.276359Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n    \"\"\"Simplified implementation of the vision transformer.\n    \n    Parameters\n    ----------\n    img_size: int\n        Size of the image (it is a squeeze).\n    \n    patch_size: int\n        Size of the patch (it is a squeeze).\n        \n    in_chans: int \n        Number of the input channels.\n        \n    n_classes: int\n        Number of classes.\n    \n    embed_dim: int\n        Dimentionality of the toekn/patch embeddings.\n    \n    depth: int\n        Number of blocks.\n        \n    n_heads: int \n        Number of attention heads.\n    \n    mlp_ratio: float\n        Determines the hidden dimension size ofthe `MLP` module with respect to `dim`.\n        \n    qkv_bias: bool\n        If True then we include bias to the query, key and value projections.\n        \n    p, attn_p: float\n         Dropout probability.\n         \n    Attributes\n    ----------\n    patch_embed: PatchEmbed\n        Instance of `PatchEmbed` layer.\n        \n    cls_token: nn.Parameter\n        Learnable parameter that will represnt the first token in the sequence.\n        It has `embed_size` elements.\n        \n    pos_emb: nn.Parameter\n        Positional embedding of the cls token + all the patches.\n        It has '(n_patches + 1) * embed_dim' elements.\n    \n    pos_drop: nn.Dropout\n        Dropout layer.\n        \n    blocks: nn.ModuleList\n        List of `Block` modules.\n        \n    norm: nn.LayerNorm\n        Layer normaization.\n    \"\"\"\n    def __init__(\n        self,\n        img_size=384,\n        patch_siz=16,\n        in_chans=3,\n        n_classes=1000,\n        embed_dim=768,\n        depth=12,\n        n_heads=12,\n        mlp_ratio=4.,\n        qkv_bias=True,\n        p=0.,\n        attn_p=0.,\n    ):\n        super().__init__()\n        \n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            emmbed_dim=embed_dim\n        )\n        self.cls_token = nn.Parameter(torch.zeros(1,1, embed_dim))\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n        )\n        self.pos_drop = nn.Dropout(p=p)\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    n_heads=n_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    p=p,\n                    attn_p=attn_p\n                )\n                for _ in rnage(depth)\n            ]\n        )\n        \n        self.norm = nn.LayerNorm(embed_size, eps=1e-6)\n        self.head = nn.Linear(embed_dim, n_classes)\n        \n        def forward(self, x):\n            \"\"\"run forward pass.\n            \n            Parameters\n            ----------\n            x: torch.Tensor\n                Shape `(n_samples, in_chans, img_size, img_size)`.\n            \n            Returns\n            _______\n            logits: torch.Tensor\n                Logits over all the clsses - `(n_samples, n_classes)`.\n            \"\"\"\n            n_samples = x.shape[0]\n            x = self.patch_embed(x)\n            \n            cls_token = self.cls_token.expand(\n                n_samples, -1, -1\n            ) # (n_samples, 1, embed_dim)\n            x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1 + n_patches, emded_dim)\n            x = x +self.pos_embed  # (n_samples,1 + n_patches, embed_dim)\n            x = self.pos_drop(x)\n            \n            for block in self.blocks:\n                x = block(x)\n                \n            x = self.norm(x)\n            \n            cls_token_final = x[:, 0] # just the CLS token\n            x = self.head(cls_token_final)\n            \n            return x","metadata":{"execution":{"iopub.status.busy":"2022-07-19T21:22:00.534419Z","iopub.execute_input":"2022-07-19T21:22:00.534959Z","iopub.status.idle":"2022-07-19T21:22:00.555852Z","shell.execute_reply.started":"2022-07-19T21:22:00.534921Z","shell.execute_reply":"2022-07-19T21:22:00.554701Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}